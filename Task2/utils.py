import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from torchmetrics.image.fid import FrechetInceptionDistance
import numpy as np

import matplotlib.pyplot as plt
import matplotlib.animation as animation
from IPython.display import HTML

from architectures import GAN, VAE, vae_loss

device = "cuda" if torch.cuda.is_available() else "cpu"

class LinearInterpolator:
    """
    Example Usage:
    (All Dims)
    vae_interpolator = LinearInterpolator(vae_model, device)
    z1 = torch.randn(vae_model.latent_dim)
    z2 = torch.randn(vae_model.latent_dim)
    video = vae_interpolator.inter(z1, z2)

    (Specific Dims (example = first dim))
    gan_interpolator = LinearInterpolator(gan_model, device)
    z1 = torch.randn(gan_model.latent_dim)
    z2 = torch.randn(gan_model.latent_dim)
    video = gan_interpolator.inter(z1, z2, vary_dims=[0])

    Make sure vary_dims is passed valid dimensions (0 <= dim < latent_dim)
    """
    def __init__(self, model: VAE | GAN, device='cpu'):
        self.model = model.to(device)
        self.device = device
        self.model.eval()

    def inter(self, z1, z2, vary_dims=None):
        z1 = z1.clone().to(self.device)
        z2 = z2.clone().to(self.device)

        if vary_dims is None:
            vary_dims = list(range(z1.shape[0]))

        latent_codes = []
        
        for alpha in np.linspace(0, 1, num=128):
            z = z1.clone()
            z[vary_dims] = z1[vary_dims] * (1-alpha) + z2[vary_dims] * alpha
            latent_codes.append(z)
        latent_codes = torch.stack(latent_codes).to(self.device)

        with torch.inference_mode():
            if isinstance(self.model, VAE):
                decoded = self.model.decode(latent_codes).reshape(-1, 3, 32, 32).cpu()
            else:
                decoded = self.model.generate(latent_codes).reshape(-1, 3, 32, 32).cpu()

        decoded = decoded.permute(0, 2, 3, 1)
        
        fig, ax = plt.subplots()
        ims = [[ax.imshow(im, animated=True)] for im in decoded.numpy()]
        ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True,repeat_delay=1000)
        plt.close(fig)
        return HTML(ani.to_html5_video())

class GANPCA:
    pass

class GAN_Inversion:
    """
    GAN_Inversion performs inversion of real images into a GAN’s
    latent space. 

    GANs map latent vectors (z) to images, but inversion solves
    the reverse problem: given a real image, find the latent code
    such that the GAN’s generator can reconstruct that image.

    Why it matters:
        - Enables editing of real images via latent manipulations
          (e.g., change color, add attributes).
        - Helps in image restoration tasks (inpainting,
          super-resolution).
        - Allows analysis of how well the GAN latent space captures
          real-world image distributions.

    Methods:
        - Optimization-based inversion: iteratively adjusts latent
          vectors to minimize the difference between the generated
          and target image.
        - Encoder-based inversion: trains an auxiliary encoder to
          directly predict latent vectors from images.
        - Hybrid approaches: combine encoder initialization with
          optimization refinement.

    In short, GAN inversion bridges the gap between real images
    and the GAN’s imagination space, enabling reconstruction and
    controllable editing.
    """
    pass

class GAN_FID:
    """
    GAN_FID evaluates the quality of images generated by a GAN
    using the Frechet Inception Distance (FID).

    FID measures the similarity between the distribution of
    generated images and real images. It works by:
        1. Passing both real and generated images through a
           pretrained Inception-v3 network.
        2. Extracting activations (features) from a chosen layer.
        3. Modeling these features as multivariate Gaussians.
        4. Computing the Frechet distance between the two
           Gaussian distributions.

    A lower FID score indicates that the generated images are
    more similar to the real images in terms of distribution,
    thus reflecting better image quality and diversity.

    Usage:
        - Initialize the class with a dataset (e.g., CIFAR-10).
        - Generate a batch of images using a trained GAN.
        - Compute FID against real images from the dataset.

    This is a standard benchmark for evaluating GANs.
    """
    def __init__(self, generator_path, device, latent_dim=128, batch_size=64, num_fake=1000):
        self.device = device
        self.batch_size = batch_size
        self.num_fake = num_fake
        self.latent_dim = latent_dim

        # Data transforms for CIFAR10
        transform = transforms.Compose([
            transforms.Resize(32),
            transforms.ToTensor(),
        ])

        cifar_test = datasets.CIFAR10(
            root="./data", train=False, transform=transform, download=True
        )
        self.test_loader = DataLoader(
            cifar_test, batch_size=batch_size, shuffle=False
        )

        self.GAN = GAN(latent_dim=latent_dim).to(self.device)
        self.GAN.load_state_dict(torch.load(generator_path, map_location=self.device))
        self.GAN.eval()

        self.fid = FrechetInceptionDistance(feature=2048).to(self.device)

    def compute_fid(self):
        self.fid.reset()

        for imgs, _ in self.test_loader:
            imgs = (imgs * 255).to(torch.uint8).to(self.device)
            self.fid.update(imgs, real=True)

        with torch.no_grad():
            for _ in range(self.num_fake // self.batch_size):
                z = torch.randn(self.batch_size, self.latent_dim, device=self.device)
                fake_imgs = self.GAN.generate(z)

                # Rescale from [-1,1] → [0,255]
                fake_imgs = ((fake_imgs + 1) / 2 * 255).clamp(0, 255)
                fake_imgs = fake_imgs.to(torch.uint8)

                self.fid.update(fake_imgs, real=False)

        score = self.fid.compute().item()
        print(f"FID score for GAN: {score:.4f}")
        return score
